{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trash Classification  Imageclassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYql7izIuX_L",
        "outputId": "fdcede61-ee74-491c-ba24-a9d589c7ce16"
      },
      "source": [
        "! pip install pyspark\n",
        "from pyspark.sql import *\n",
        "spark = SparkSession.builder \\\n",
        "   .master(\"local[*]\") \\\n",
        "   .appName(\"ImageClassification\") \\\n",
        "   .config(\"spark.executor.memory\", \"16gb\") \\\n",
        "   .config(\"spark.driver.memory\", \"16G\") \\\n",
        "   .config(\"spark.driver.offHeap.enabled\", \"true\") \\\n",
        "   .config(\"spark.driver.offHeap.size\", \"16G\") \\\n",
        "   .config(\"spark.executor.maxResultSize\", \"16gb\") \\\n",
        "   .getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 26kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 51.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=bb1e394242ea448013d7168d5805ff295c03f58d133f874e757be3a5ef56b2fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgPsoOfv2JUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75075986-afad-44cb-d057-15c91da04679"
      },
      "source": [
        "# innstall java\n",
        "!apt-get update\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (91.189.91.39)] [Connecting to cloud.r-pr\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,759 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [900 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,550 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,182 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [429 kB]\n",
            "Get:21 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Get:22 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [47.2 kB]\n",
            "Get:23 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [53.9 kB]\n",
            "Ign:25 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:25 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [770 kB]\n",
            "Get:26 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
            "Get:27 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,410 kB]\n",
            "Get:28 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [399 kB]\n",
            "Get:29 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,119 kB]\n",
            "Fetched 13.0 MB in 3s (4,070 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGTuj_fs2JSS"
      },
      "source": [
        "os.environ[\"SPARK_HOME\"]\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NENtNEDX3_2u",
        "outputId": "d9ab8bca-50af-4ac1-ee00-97db40be972c"
      },
      "source": [
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbnKiyxDxyRM",
        "outputId": "2964613b-2ea9-4084-9a46-4fe17f73c502"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "q3P3dapHu3Um",
        "outputId": "b012bcab-f792-4ff1-d2a1-1da7cc09e124"
      },
      "source": [
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f325631932d0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ImageClassification</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=ImageClassification>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "966wRXzzu3Rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b6e473-5f6a-4758-e519-f7600a172fc8"
      },
      "source": [
        "\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9OhkzDIu3LN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4713fdb-be5e-466d-cde4-a9ef98a48372"
      },
      "source": [
        "\n",
        "import pyspark.sql.functions as f\n",
        "! pip install sparkdl\n",
        "! pip install tensorframes\n",
        "! pip install kafka-python\n",
        "! pip install tensorflowonspark\n",
        "! pip install pillow\n",
        "! pip install nose \n",
        "! pip install h5py\n",
        "! pip install py4j\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sparkdl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/e6/c520f801b945f3d03dbf47e1abb7a454cda328d1592f9854dcec69bed097/sparkdl-0.2.2-py3-none-any.whl (99kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20kB 25.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 40kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 61kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 71kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 81kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 92kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 6.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sparkdl\n",
            "Successfully installed sparkdl-0.2.2\n",
            "Collecting tensorframes\n",
            "  Downloading https://files.pythonhosted.org/packages/59/ae/e8607d0bc5d722694250dcfce59cc9d530e93406079c7d1bf4cb4bbe9d9a/tensorframes-0.2.9-py3-none-any.whl\n",
            "Installing collected packages: tensorframes\n",
            "Successfully installed tensorframes-0.2.9\n",
            "Collecting kafka-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/68/dcb0db055309f680ab2931a3eeb22d865604b638acf8c914bedf4c1a0c8c/kafka_python-2.0.2-py2.py3-none-any.whl (246kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 13.3MB/s \n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n",
            "Collecting tensorflowonspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/b3/c57e0a79f4a5d43948e0871707f96208b34bde4827345dd75acbfd72ed17/tensorflowonspark-2.2.3-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>38.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowonspark) (56.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflowonspark) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflowonspark) (2.4.7)\n",
            "Installing collected packages: tensorflowonspark\n",
            "Successfully installed tensorflowonspark-2.2.3\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 10.0MB/s \n",
            "\u001b[?25hInstalling collected packages: nose\n",
            "Successfully installed nose-1.3.7\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwMCPWyqCDlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f2390e-b272-4aee-815e-ab06a8a1de2c"
      },
      "source": [
        "import pyspark # pyspark library\n",
        "import sparkdl as dl # deep learning library : tensorflow backend \n",
        "from keras.applications import InceptionV3 # transfer learning using pyspark"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sdf7x1CLHKz"
      },
      "source": [
        "import os\n",
        "\n",
        "SUBMIT_ARGS = \"--packages databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
        "\n",
        "\n",
        "import sys, glob, os\n",
        "sys.path.extend(glob.glob(os.path.join(os.path.expanduser(\"~\"), \".ivy2/jars/*.jar\")))\n",
        "from pyspark.ml.image import ImageSchema\n",
        "from pyspark.sql.functions import lit"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gauG7jUlu3Ij"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTNyalH25Tzf"
      },
      "source": [
        "import sys, glob, os\n",
        "sys.path.extend(glob.glob(os.path.join(os.path.expanduser(\"~\"), \".ivy2/jars/*.jar\")))\n",
        "\n",
        "from pyspark.ml.image import ImageSchema\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import lit\n",
        "from sparkdl.image import imageIO\n",
        "\n",
        "cardboard_df = spark.read.format(\"image\").load(\"/content/drive/MyDrive/sample/cardboard\").withColumn(\"label\", lit(0))\n",
        "plastic_df = spark.read.format(\"image\").load( \"/content/drive/MyDrive/sample/glass\").withColumn(\"label\", lit(1))\n",
        "paper_df = spark.read.format(\"image\").load(\"/content/drive/MyDrive/sample/metal\").withColumn(\"label\", lit(2))\n",
        "trash_df = spark.read.format(\"image\").load(\"/content/drive/MyDrive/sample/paper\").withColumn(\"label\", lit(3))\n",
        "glass_df = spark.read.format(\"image\").load( \"/content/drive/MyDrive/sample/plastic\").withColumn(\"label\", lit(4))\n",
        "metal_df = spark.read.format(\"image\").load(\"/content/drive/MyDrive/sample/trash\").withColumn(\"label\", lit(5))\n",
        "\n",
        "\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "dataframes = [cardboard_df,plastic_df, paper_df, trash_df, \n",
        "             glass_df,glass_df,metal_df]\n",
        "\n",
        "\n",
        "df1 = reduce(lambda first, second: first.union(second), dataframes)\n",
        "\n",
        "# repartition dataframe \n",
        "df1 = df1.repartition(200)\n",
        "\n",
        "\n",
        "# # On hot encoding \n",
        "# from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "# encoder = OneHotEncoderEstimator(inputCols=[\"label\"],outputCols=[\"one_hot_label\"])\n",
        "# model = encoder.fit(df)\n",
        "# df = model.transform(df)\n",
        "\n",
        "# split the data-frame\n",
        "train, test = df1.randomSplit([0.6, 0.4], 42)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "K7pxg2u46wiL",
        "outputId": "84a21f09-18ed-4c0d-e436-e3b0334dc8eb"
      },
      "source": [
        "train.toPandas().head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(file:///content/drive/MyDrive/sample/plastic/...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(file:///content/drive/MyDrive/sample/plastic/...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(file:///content/drive/MyDrive/sample/plastic/...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(file:///content/drive/MyDrive/sample/glass/gl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(file:///content/drive/MyDrive/sample/glass/gl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               image  label\n",
              "0  (file:///content/drive/MyDrive/sample/plastic/...      4\n",
              "1  (file:///content/drive/MyDrive/sample/plastic/...      4\n",
              "2  (file:///content/drive/MyDrive/sample/plastic/...      4\n",
              "3  (file:///content/drive/MyDrive/sample/glass/gl...      1\n",
              "4  (file:///content/drive/MyDrive/sample/glass/gl...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d-WnrTSyJKN"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from sparkdl import DeepImageFeaturizer \n",
        "\n",
        "# model: InceptionV3\n",
        "# extracting feature from images\n",
        "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\",\n",
        "\n",
        "                                 modelName=\"InceptionV3\")\n",
        "\n",
        "# used as a multi class classifier\n",
        "lr = LogisticRegression(maxIter=5, regParam=0.03, \n",
        "                        elasticNetParam=0.5, labelCol=\"label\") \n",
        "\n",
        "# define a pipeline model\n",
        "sparkdn = Pipeline(stages=[featurizer, lr])\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjjgFT3oT7zs"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "import sparkdl\n",
        "from sparkdl import DeepImageFeaturizer \n",
        "\n",
        "# model: InceptionV3\n",
        "# extracting feature from images\n",
        "featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\",modelName=\"InceptionV3\")\n",
        "\n",
        "# used as a multi class classifier\n",
        "lr = LogisticRegression(maxIter=5, regParam=0.03, \n",
        "                        elasticNetParam=0.5, labelCol=\"label\") \n",
        "\n",
        "# define a pipeline model\n",
        "sparkdn = Pipeline(stages=[featurizer, lr])\n",
        "\n",
        "spark_model = sparkdn.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QVpU6Na6vvS"
      },
      "source": [
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "# evaluate the model with test set\n",
        "evaluator = MulticlassClassificationEvaluator() \n",
        "transform_test = spark_model.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr2tzQ7A4fbJ"
      },
      "source": [
        "print('F1-Score ', evaluator.evaluate(transform_test, \n",
        "                                      {evaluator.metricName: 'f1'}))\n",
        "print('Precision ', evaluator.evaluate(transform_test,\n",
        "                                       {evaluator.metricName: 'weightedPrecision'}))\n",
        "print('Recall ', evaluator.evaluate(transform_test, \n",
        "                                    {evaluator.metricName: 'weightedRecall'}))\n",
        "print('Accuracy ', evaluator.evaluate(transform_test, \n",
        "                                      {evaluator.metricName: 'accuracy'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xTedHzU04sm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.GnBu):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V04oeff3yJG-"
      },
      "source": [
        "'''\n",
        "- Convert Spark-DataFrame to Pnadas-DataFrame\n",
        "- Call Confusion Matrix With 'True' and 'Predicted' Label\n",
        "'''\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_true = transform_test.select(\"label\")\n",
        "y_true = y_true.toPandas() # convert to pandas dataframe from spark dataframe\n",
        "\n",
        "y_pred = transform_test.select(\"prediction\")\n",
        "y_pred = y_pred.toPandas() # convert to pandas dataframe from spark dataframe\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_true, y_pred,labels=range(6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh133vRAyJEg"
      },
      "source": [
        "'''\n",
        "- Visualize the 'Confusion Matrix' \n",
        "'''\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.grid(False)\n",
        "\n",
        "# call pre defined function\n",
        "plot_confusion_matrix(cnf_matrix, classes=range(6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leQ9MbYPyJBS"
      },
      "source": [
        "'''\n",
        "- Classification Report of each class group\n",
        "'''\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(6)]\n",
        "print(classification_report(y_true, y_pred, target_names = target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuUPMnhCCPbo"
      },
      "source": [
        "\n",
        "'''\n",
        "- A custom ROC AUC score function for multi-class classification problem\n",
        "'''\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "\n",
        "print('ROC AUC score:', multiclass_roc_auc_score(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmzG1L2xCPRE"
      },
      "source": [
        "\n",
        "'''\n",
        "- Comparing true vs predicted samples\n",
        "'''\n",
        "\n",
        "# all columns after transformations\n",
        "print(transform_test.columns)\n",
        "\n",
        "# see some predicted output\n",
        "transform_test.select('image', \"prediction\", \"label\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tckQI2SqCUzD",
        "outputId": "ad94277a-ac57-4c23-9e36-c6d3274af361"
      },
      "source": [
        "!pip install bigdl==0.3.0 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bigdl==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/79/8a4f3d74fbaa0b7722c2c872bcd06ce864a3ab0a1cabc2890f3bed06e223/BigDL-0.3.0-py2.py3-none-manylinux1_x86_64.whl (59.4MB)\n",
            "\u001b[K     |████████████████████████████████| 59.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyspark>=2.2 in /usr/local/lib/python3.7/dist-packages (from bigdl==0.3.0) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bigdl==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark>=2.2->bigdl==0.3.0) (0.10.9)\n",
            "Installing collected packages: bigdl\n",
            "Successfully installed bigdl-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJSrSw_FZTu9",
        "outputId": "68fdc721-f567-4b1e-e765-0d63bfcd43bc"
      },
      "source": [
        "! pip install utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjh5OX4JYk5v"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNhQiWQ2YtZj"
      },
      "source": [
        "# Parameters\n",
        "training_epochs = 10\n",
        "batch_size = 128\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden = 32\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPEk78H-YtWz"
      },
      "source": [
        "# Create Model\n",
        "\n",
        "def build_autoencoder(n_input, n_hidden):\n",
        "    # Initialize a sequential container\n",
        "    model = Sequential()\n",
        "\n",
        "    # encoder\n",
        "    model.add(Linear(n_input, n_hidden))\n",
        "    model.add(ReLU())\n",
        "    # decoder\n",
        "    model.add(Linear(n_hidden, n_input))\n",
        "    model.add(Sigmoid())\n",
        "    \n",
        "    return model\n",
        "model = build_autoencoder(n_input, n_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TpAhXSEYtLY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}